---
title: "Module 3 Assignment on Classification"
author: "Xubin Lou // Undergraduate Student"
date: "03/03/2021"
#output: pdf_document
output:
  pdf_document: default
  df_print: paged
  #html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=80))
```

***

\newpage{}


***
## Module Assignment Questions
## Q1) (*Bayes Classifier*) 

`Bayes classifier` classifies an observation $x_0$ to the class $k$ for which $p_k(x_0)$ is largest, where $\pi_k$ is prior (proportion of $k$ class in all classes over $j$):

$$
p_k(x_0) = P(y=k | X=x_0) = \frac {\pi_k \cdot f_k(x_0)} {\sum { \pi_j \cdot f_j(x_0)}}.
$$

Assume univariate (p=1) observation $x$ in class $k$ is iid from $N(\mu_k, \sigma_k^2)$, $f_k(x)$ is the density function of $x$ with parameters $\mu_k,\sigma_k$.

a. Show that the Bayes classifier in 2-class problem (so $k=0,1$) assigns the observation $x_0$ to the class $k$ for which the discriminant score $\delta$ is largest when $\sigma_0=\sigma_1$ :

$$
\delta_k(x_0) = x_0 \frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2 \sigma^2} + \log(\pi_k)
$$ 

b. (Empirical Work) Verify `part a` with a simple empirical demonstration using normal densities in R with `dnorm()` or  generated normal variables from `rnorm()` with $\mu_0 = 10, \mu_1=15, \sigma_0=\sigma_1=2, \pi_0 = 0.3, \pi_1=.7, mu2 = 15, pi2 = 0.7$. Plot the class densities or histograms in color, show the intersection between two class distributions (where the classification boundary starts), check one random value from each class by calculating the discriminant score so to confirm the class it belongs. How would you describe the misclassified values or regions? Calculate the error rate. What is the Bayes error rate?

c. Under `part a`, assume $\sigma_0 \neq \sigma_1$.  Derive the Bayes classifier. Show work.

d. (BONUS) For p>1, derive the the Bayes classifier. Show work.


***
## Q2) (*Four Models as Classifiers*) 

The `Boston` data from `MASS` has 506 rows and 14 columns with the target variable `crim`, which is per capita crime rate by town. You will fit classification models (`KNN`, `logistic regression`, `LDA`, and `QDA`) in order to predict whether a given suburb has a crime rate above or below .3 per capita crime rate by town. Upper .3 may be labeled as `not really safe town` to raise a family. Use `80%-20% split validation test` approach.    


a. Fit the `KNN`, `logistic regression`, `LDA`, and `QDA` models separately using all the predictors. Report  `error rate` for each `train` and `test` data set. Use `error rate` = `1-accuracy`. Based on the test error rates, decide which model is best/better. Why?

b. Using the test data set, obtain confusion matrices and report only `recall`, `precision`, `f1` and `accuracy` metrics in a table. Comment on the findings. Based on this table, decide which model is best/better. Explain why some models do better than others. Which metric would be most important in this context? Why? Is this decision different from that of `part a`? Explain.

c. Obtain the ROC curve for `logistic regression` based on train data set (plot of FPR vs TPR with classification threshold change). Plot it. Calculate the area under the curve. Explain what the curve and area tell about the model. 

d. How did you find the optimal $k$ in the `KNN` classifier? Did you use `grid search` or `CV`? If not, use it and revise the results in part a and b. Did the results improve?

e. What are the assumptions in each model? Do your best to describe each. Do your best to check these based on the fit. When you see assumption violation, what would you do to validate the fit?



***
## Q3) (*Concepts*) 

a. What would change if you perform `$k-$fold approach` instead of `validation set` approach for the model fits in Question 2? Just discuss conceptually.

b. To improve the test error rates in `part a` Q2, what strategies can be applied: list the ideas as many as possible. Try one of them and report the improved test error rate.

c. Explain with less technical terms an estimation method employed in `binary logistic regression`. `MLE` and `gradient descent` are two of them. 

d. (BONUS) Demonstrate with technical terms and numerically how `MLE` as estimation method employed in `binary logistic regression` works. Explain.  


***
Review the R lab codes. Also, some useful codes to start are here:

```{r eval=FALSE}
#Some useful codes
library(MASS)
summary(Boston)
rm(Boston)
attach(Boston)
str(Boston)
dim(Boston)
n = nrow(Boston)
n
hist(crim)
summary(crim)
crime_dummy = rep(0, length(crim))
#Q3 is 1
quantile(crim, .75)
crime_dummy[crim>1] = 1
Boston = data.frame(Boston, crime_dummy)
View(Boston)
#rate in crime_dummy is 0.2509881 (P)
sum(crime_dummy)/length(crime_dummy)
# choose randomly 80% 
set.seed(99)
train=sample(c(TRUE,FALSE), size=n, 
             prob=c(.80, .20), rep=TRUE) #randomly select index whether train or not from row
test=(!train)
Boston.train = Boston[train,]
dim(Boston.train)
Boston.test = Boston[test,]
dim(Boston.test)
crime_dummy.train = crime_dummy[train]
sum(crime_dummy.train)/length(crime_dummy.train)
crime_dummy.test = crime_dummy[test]
sum(crime_dummy.test)/length(crime_dummy.test)
# (this is another option to split the data into train and test)
n_train = ceiling(.80 * n)
n_train
```

\newpage

***


## Your Solutions

Q1) 

Part a:

I've written down my process by hand in the submitted pdf, please check Module3_HWQ1-parta&d.pdf. The final expression is:
$$
x_0 \frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2 \sigma^2} + \log(\pi_k)
$$

***
Part b:
```{r}
set.seed(99)
mean0 = 10
mean1 = 15
pi0 = 0.3
pi1 = 0.7
sd0= 2
sd1=2
x1 = rnorm(100*pi0, mean = mean0, sd = sd0)
x2 = rnorm(100*pi1, mean = mean1, sd = sd1)
plot(hist(x1,plot=FALSE), col = '#90ee90' ,xlim = c(0,20),ylim = c(-1,25))
plot(hist(x2,plot=FALSE), col = rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink") , add = T)
abline(v=15,col="red")

set.seed(99)
p0_1 = sample(x1,1) * (mean0/sd0^2) - (mean0^2/(2*sd0^2)) + log(pi0)
cat('\np0_1 is:', p0_1)
p1_1 = sample(x1,1) * (mean1/sd1^2) - (mean1^2/(2*sd1^2)) + log(pi1)
cat('\np0_1 is:', p1_1)
p0_1/p1_1
cat('\np0_1/p1_1 is:', p0_1/p1_1)
cat("\nSince p0_1/p1_1>0, x1 belongs to class k = 0.")
p0_2 = sample(x2,1) * (mean0/sd0^2) - (mean0^2/(2*sd0^2)) + log(pi0)
cat('\np0_2 is:', p0_2)
p1_2 = sample(x2,1) * (mean1/sd1^2) - (mean1^2/(2*sd1^2)) + log(pi1)
cat('\np1_2 is:', p1_2)
p0_2/p1_2
cat('\np0_2/p1_2 is:', p0_2/p1_2)
cat("\nSince p0_2/p1_2<0, x2 belongs to class k = 1.\n")
cat("As shown in the graoh, the intersection between two class distributinos is about from x=10 to x=14. \nThe misclassified values might be at the intersection region of the two classes \nbecause they have closed p values from each class.\n")

P0fun <- function(x){
  p0_fun = pi0*( (1/(sqrt(2*pi)*sd0))*exp(-(1/(2*sd0^2))*(x-mean0)^2) ) / ((pi0*( (1/(sqrt(2*pi)*sd0))*exp(-(1/(2*sd0^2))*(x-mean0)^2) ))+(pi1*( (1/(sqrt(2*pi)*sd1))*exp(-(1/(2*sd1^2))*(x-mean1)^2) )))
  return(p0_fun)
}
P1fun <- function(x){
  p1_fun = pi1*( (1/(sqrt(2*pi)*sd1))*exp(-(1/(2*sd1^2))*(x-mean1)^2) ) / ((pi0*( (1/(sqrt(2*pi)*sd0))*exp(-(1/(2*sd0^2))*(x-mean0)^2) ))+(pi1*( (1/(sqrt(2*pi)*sd1))*exp(-(1/(2*sd1^2))*(x-mean1)^2) )))
  return(p1_fun)
}
X <- c(x1,x2)
Pr = vector(length = 100)
for (i in (1:100)) {
  Pr[i] = max(P0fun(X[i]),P1fun(X[i]))
}
bayes_error = 1-mean(Pr)
cat("\nThe Bayes Error Rate is",bayes_error)
```

***
Part c:

As now the variances are not the same, the ignored terms in part (a) will be not ignored in part (c). So, the expression for unequal variance bayes classifier is:
$$
log(\pi_k)+log(\frac{1}{\sqrt{2\pi}\sigma_k})-\frac{x_0^2}{2\sigma_k^2}+x_0\frac{\mu_k}{\sigma_k^2}-\frac{\mu_k^2}{2\sigma_k^2}
$$

***
Part d (BONUS):

I've written down my process by hand in the submitted pdf, please check Module3_HWQ1-parta&d.pdf. The final expression is:
$$
log(\pi_k)+x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k
$$

***



\newpage

Q2) 

Part a:
```{r}
library(MASS)
library(class)
attach(Boston)
n = nrow(Boston)
dim(Boston)
crime_dummy = rep(0, length(crim))
crime_dummy[crim>0.3] = 1
set.seed(99)
train=sample(c(TRUE,FALSE), size=n, prob=c(.80, .20), rep=TRUE)
test=(!train)
Boston.train = Boston[train,]
Boston.test = Boston[test,]
crime_dummy.train = crime_dummy[train]
crime_dummy.test = crime_dummy[test]

perfcheck <- function(ct) {
  Accuracy <- (ct[1]+ct[4])/sum(ct)
  Recall <- ct[4]/sum((ct[2]+ct[4]))      #TP/P   or Power, Sensitivity, TPR 
  Type1 <- ct[3]/sum((ct[1]+ct[3]))       #FP/N   or 1 - Specificity , FPR
  Precision <- ct[4]/sum((ct[3]+ct[4]))   #TP/P*
  Type2 <- ct[2]/sum((ct[2]+ct[4]))       #FN/P
  F1 <- 2/(1/Recall+1/Precision)
  Values <- as.vector(round(c(Accuracy, Recall, Type1, Precision, Type2, F1),4)) *100
  Metrics = c("Accuracy", "Recall", "Type1", "Precision", "Type2", "F1")
  cbind(Metrics, Values)
  #list(Performance=round(Performance, 4))
}

#KNN
train.X = cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat,
    medv)[train, ]
test.X = cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat,
    medv)[test, ]
dim(train.X)
dim(test.X)
set.seed(1)
#train group
knn.pred.train = knn(test.X, train.X, crime_dummy.test, k = 1)
table(crime_dummy.train, knn.pred.train)
ct1 = table(crime_dummy.train, knn.pred.train)
perfcheck(ct1)
cat("\nThe train error rate for KNN is ", (1 - (ct1[1] + ct1[4])/409) * 100, "%.")
# test group
knn.pred.test = knn(train.X, test.X, crime_dummy.train, k = 1)
table(crime_dummy.test, knn.pred.test)
ct2 = table(crime_dummy.test, knn.pred.test)
perfcheck(ct2)
cat("\nThe test error rate for KNN is ", (1 - (ct2[1] + ct2[4])/97) * 100, "%.")

#logistic regression
#train group
attach(Boston)
glm.fits.train=glm(crime_dummy.train~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv,data=Boston.train,family=binomial)
glm.probs.train=predict(glm.fits.train, Boston.train, type="response")
length(glm.probs.train)
glm.pred.train=rep("<0.3",409)
glm.pred.train[glm.probs.train>.5]=">0.3"
table(crime_dummy.train, glm.pred.train)
ct3<-table(crime_dummy.train, glm.pred.train)
perfcheck(ct3)
cat("The train error rate for logistic regression is",(1 - (ct3[1] + ct3[4])/409) *100, "%.")
#test group
glm.fits.test=glm(crime_dummy.test~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv,data=Boston.test,family=binomial)
glm.probs.test=predict(glm.fits.train, Boston.test, type="response")
length(glm.probs.test)
glm.pred.test=rep("<0.3",97)
glm.pred.test[glm.probs.test>.5]=">0.3"
table(crime_dummy.test, glm.pred.test)
ct4<-table(crime_dummy.test, glm.pred.test)
perfcheck(ct4)
cat("The test error rate for logistic regression is",(1 - (ct4[1] + ct4[4])/97) *100, "%.")

#LDA
#train group
lda.fit.train=lda(crime_dummy.train~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv,data=Boston.train)
lda.pred.train=predict(lda.fit.train, Boston.train)
lda.train.class=lda.pred.train$class
table(crime_dummy.train,lda.train.class)
ct5<-table(crime_dummy.train,lda.train.class)
perfcheck(ct5)
cat("The train error rate for LDA is",(1 - (ct5[1] + ct5[4])/409) * 100, "%.")
#test group
lda.pred.test = predict(lda.fit.train, Boston.test)
lda.test.class = lda.pred.test$class
table(crime_dummy.test, lda.test.class)
ct6 = table(crime_dummy.test, lda.test.class)
perfcheck(ct6)
cat("\nThe test error rate for LDA is ", (1 - (ct6[1] + ct6[4])/97) * 100, "%.")

#QDA
#train group
qda.fit.train=qda(crime_dummy.train~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv,data=Boston.train)
qda.train.class=predict(qda.fit.train,Boston.train)$class
table(crime_dummy.train, qda.train.class)
ct7<-table(crime_dummy.train, qda.train.class)
perfcheck(ct7)
cat("The train error rate for QDA is 1-precision=",(1 - (ct7[1] + ct7[4])/409) * 100, "%.")
#test group
qda.test.class=predict(qda.fit.train, Boston.test)$class
table(crime_dummy.test, qda.test.class)
ct8<-table(crime_dummy.test, qda.test.class)
perfcheck(ct8)
cat("The test error rate for QDA is 1-precision=",(1 - (ct8[1] + ct8[4])/97) * 100, "%.")
```
By comparing the test error rates we get form each model above, we find that here KNN model has the smallest test error rate, thus the KNN model is best.

***
Part b:
```{r}
#KNN
cat('Confusion matrix of KNN')
perfcheck(ct2)
#logistic regression
cat('Confusion matrix of Logistic regression')
perfcheck(ct4)
#LDA
cat('Confusion matrix of LDA')
perfcheck(ct6)
#QDA
cat('Confusion matrix of QDA')
perfcheck(ct8)

#table
accu1=perfcheck(ct2)[1,2]
accu2=perfcheck(ct4)[1,2]
accu3=perfcheck(ct6)[1,2]
accu4=perfcheck(ct8)[1,2]
recall1=perfcheck(ct2)[2,2]
recall2=perfcheck(ct4)[2,2]
recall3=perfcheck(ct6)[2,2]
recall4=perfcheck(ct8)[2,2]
preci1=perfcheck(ct2)[4,2]
preci2=perfcheck(ct4)[4,2]
preci3=perfcheck(ct6)[4,2]
preci4=perfcheck(ct8)[4,2]
f1_1=perfcheck(ct2)[6,2]
f1_2=perfcheck(ct4)[6,2]
f1_3=perfcheck(ct6)[6,2]
f1_4=perfcheck(ct8)[6,2]

KNN = as.matrix(c(accu1,recall1,preci1,f1_1)) 
Logistic_reg = c(accu2,recall2,preci2,f1_4)
LDA = c(accu3,recall3,preci3,f1_3)
QDA = c(accu4,recall4,preci4,f1_4)
E = cbind(KNN,Logistic_reg,LDA,QDA)
colnames(E) = c("KNN", "Logistic regression", "LDA", "QDA") 
rownames(E) = c("Acurracy", "Recall", "Precision", "F1")
E
# Lets use a better format from table
knitr::kable(E, caption = "Table of metrics for the models")
```
KNN model is the best. We can see that in the table above, the model KNN has the largest value of accuracy, recall, precision, and F1, compared with other models. F1 metric is the most important because it combines the other two metrics together to evaluate models in a more comprehensive way. This result is the same as that in part a.

***
Part c:
```{r}
library(ROCR)
glm.prob.train<-predict(glm.fits.train, Boston.train,type="response")
plot(performance(ROCR::prediction(glm.prob.train, crime_dummy.train),"tpr","fpr"),col="Orange")
abline(0,1)
library(zoo)
x <- unlist(performance(ROCR::prediction(glm.prob.train, crime_dummy.train),"tpr","fpr")@x.values)
y <- unlist(performance(ROCR::prediction(glm.prob.train, crime_dummy.train),"tpr","fpr")@y.values)
id <- order(x)
sum(diff(x[id])*rollmean(y[id],2)) - 0.5
```
The area under curve is 0.4768133. The area tells about the classifier's usefulness. The curve tells us each threshold values' true positive rate and false positive rate . 

***
Part d:
```{r}
library(caret)
Boston.cv = Boston
Boston.cv$crim = crime_dummy
set.seed(99)
KNN.fit <- train(crim~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv, data = Boston.cv, method = "knn", trControl = trainControl(method="repeatedcv",repeats = 3), preProcess =c("center","scale"),tuneLength = 20)
KNN.fit
```
For the above result we get, the value of RMSE changes to the same direction as the k does.The smaller the k is, the smaller RMSE is. The k is better in the KNN classifier as the smaller k is. So, we can know that the smallest k is the best. Here k=1 is the best. Because I've used k=1 in part a and b, I don't need to revise the results and the results did not improve. 

***
Part e:

KNN: No assumptions.


Logistic Regression:

1. Binary response variable. (Meet: crime_dummy only has 1 and 0 (whether crim is above 0.3 or not).)

2. Absence of multicollinearity among explanatory variables. (Meet)

3. No extreme outliers. (Meet: only has 1 and 0)

4. Linearity in the logit for continuous response variables. (Not Meet. To validate the fit we should do some data transformation.)

5. Observations are independent. (Meet: The residual plot has no clear pattern.)

6. Sample size is large enough. (Meet: There are 506 observations, so it's large enough)


LDA:

1. Normality. Independent variables are normal for each level of the grouping variable. (Meet)

2. Each attribute has the equal variance. (Meet)

3. Observations are independent of each other. (Meet: Because the residual plot has no clear pattern.)


QDA: Normality. Independent variables are normal for each level of the grouping variable.

***


\newpage


Q3) 

Part a:

The error rates can be decreased and thus improve the model and get better result if I perform $k-$fold approach instead of validation set approach for the model fits in Question 2, since $k-$fold approach separates data into k folds and calculates the error rates multiple times.

***
Part b:

Strategies can be applied are data transformation, k-fold approach, and svm. The following method I use is svm.
```{r}
library(e1071)
f<- svm(crime_dummy~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv, data = Boston, type = "nu-regression",trControl = +trainControl(method = "repeatedcv"))

prob=predict(f,Boston.test,type="response")
pred=rep("<0.3",97)
pred[prob>.5]=">0.3"
ct = table(crime_dummy.test,pred)
ct
error= 1-(ct[1]+ct[4])/sum(ct) 
error
```
The test error rate is 0.06185567, and it is not improved from part a of Q2.

***
Part c:

Estimation methods are employed in the binary logistic regression to obtain the estimations of parameters. 

MLE maximizes a likelihood function to find the parameter values that give the distribution that maximize the probability of observing the data.

Gradient descent is an optimization algorithm for finding the local minimum of a differentiable function by iteratively adjusting the values in the direction of steepest descent as defined by the negative of gradient. 

***
Part d (BONUS):
$\hat{\beta}_0$ is the estimate of ${\beta}_0$; $\hat{\beta}_1$ is the estimate of ${\beta}_1$.
We try to find estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ to maximize the likelihood function: $$\ell(\beta_0,\beta_1) = \prod_{i:y_i=1} p(x_i) \prod_{i':y_{i'}=0} (1-p(x_{i'}))$$

***



\newpage

### Write comments, questions: ...


***
I hereby write and submit my solutions without violating the academic honesty and integrity. If not, I accept the consequences. 

### List the friends you worked with (name, last name): 
Jiayue Meng(partner), Rong Fan

### Disclose the resources or persons if you get any help: 
Module3_Lab_Rcodes.Rmd

### How long did the assignment solutions take?: 
About 22 hours


***
## References
...
