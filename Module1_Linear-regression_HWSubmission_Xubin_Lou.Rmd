---
title: "Module 1 Assignment on Linear Regression"
author: "Xubin Lou // Undergraduate Student"
date: "02/17/2021"
#output: pdf_document
output:
  pdf_document: default
  df_print: paged
  #html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=80))
```

***

\newpage{}


## Module Assignment Questions

## 1) (*Concepts*) 

Perform the following commands in R after you read the docs by running `help(runif)` and `help(rnorm)`:

```{r echo=TRUE, eval=FALSE}
set.seed(99)
n=100
x1=runif(n) # predictor 1
x2=rnorm(n) # predictor 2
x3=x1+x2+rnorm(n)+5 # predictor 3
y1=2 + 4* x2 + rnorm(n) # Model 1
y2=2 + 3*x1 + 4* x2 + 5*x3 + rnorm(n) # Model 2
summary(lm(y1~x2)) #Fitted Model 1
summary(lm(y2~x1+x2+x3)) #Fitted Model 2
```

The last lines correspond to creating two linear models (call Model 1 and Model 2, respectively) and their fitted results in which y1 and y2 are functions of some of the predictors x1, x2 and x3. 

a. Fit a least squares (LS) regression for Model 1. Plot the response and the predictor. Use the abline() function to display the fitted line. What are the regression coefficient estimates? Report with standard errors and p-values in a table.
b. Is the fitted Model 1 good? Do quality of model check. Justify with appropriate metrics we covered.
c. Now fit a LS regression for Model 2. What are the regression coefficient estimates? Report them along with the standard errors and p-values. Are the predictors significantly contributing to the model? Explain.
d. What is the correlation between x2 and x3? Create a scatterplot displaying the relationship between the variables. Comment on the strength of the correlation.
e. What are the assumptions in fitted Model 2? List the four assumptions. Check each. Comment on each.
f. Do you think adding the new predictors, x1 and x3, to Model 1 improved Model 1? Test it using ANOVA F method. Comment on the results.
g. Now suppose we corrupt one of the observations in y2: corrupt the first observation by adding 100 and then multiplying by 100 ($x1_1^*=100+100*x1_1$). Re-fit Model 2 using this new data. Address each question: What changed? What effect does this new observation have on the model? Is this observation an outlier on the fitted model? Is this observation a high-leverage point? Explain your answers showing fully knowledge and computations.


***

## 2) (*Application*) 

This question involves the use of multiple linear regression on the `Auto` data set on 9 variables and 392 vehicles with a dependent (target) variable `mpg`.

Variable names:

- `mpg`: miles per gallon
- `cylinders`: Number of cylinders between 4 and 8
- `displacement`: Engine displacement (cu. inches)
- `horsepower`: Engine horsepower
- `weight`: Vehicle weight (lbs.)
- `acceleration`: Time to accelerate from 0 to 60 mph (sec.)
- `year`: Model year (modulo 100)
- `origin`: Origin of car (1. American, 2. European, 3. Japanese)
- `name`: Vehicle name

A simple linear regression (SLR) model can be fitted with the code:
```{r, echo=TRUE, eval=TRUE}
library(ISLR)
attach(Auto) #this enables to use the column names
#summary(Auto) #always do EDA and graphs first
#simple linear model fit. This is 'regress y=mpg onto x=horsepower'
lm.fit = lm(mpg ~ horsepower)
#summary(lm.fit)
```

Before doing a model fit, do exploratory data analysis (EDA) by getting numerical or graph summaries. For example, the sample mean and sd of mpg is `r round(mean(mpg, na.rm=T),2)` and `r round(sd(mpg, na.rm=T),2)`. Determine types of data: If predictors are numerical, lm() will work directly; if categorical, you need to make dummy or factor() will do it.

In the SLR fitted model, the $R^2$ of the fit is `r round(summary(lm.fit)$r.sq,4)`, meaning `r round(summary(lm.fit)$r.sq,4) * 100.0`% of the variance in mpg is explained by horsepower in the linear model. 

In this part, you will fit multiple linear regression (MLR) models using the lm() with mpg as the response and all the other features as the predictor. Use the summary() function to print the results. Use the plot() function to produce diagnostic plots of the least squares regression fit. Include and comment on the output.

- Call the sample mean of mpg, `Model Baseline`.

- Perform a SLR with mpg as the response and horsepower as the predictor. Call this model, `Model 1`.

- Perform a MLR with mpg as the response and horsepower and year as the predictors. Call this model, `Model 2`.

- Perform a MLR with mpg as the response and all other variables except the categorical variables as the predictors. Call this model, `Model 3`.

- Perform a MLR with mpg as the response and all variables including the categorical variables as the predictors. Call this model, `Model Full`.

a. Produce a scatterplot matrix which includes all of the variables in the data set.
b. Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the qualitative variables.
c. What does the coefficient for the mpg variable suggest in Model 1? Does it change in other models?
d. Make a table and report $SSTO$, $MSE$, $R^2$, $R^2_{adj}$, $BIC$, $F$-ts and $F$-pvalue for each model, if applicable.
e. Comment briefly on the quality of the fit for each model. Do this in the table you created in part d.
f. Which predictors appear `most important` in the `Model Full` fit in terms of relationship to the response? How do you justify?
g. Predict the mpg at `c(horsepower, year)=c(200, 80)`. Report the 95% confidence interval for the prediction.
h. Do the fit diagnostics for the Model 2 fit by doing:
- Check some assumptions. Include necessary plots. Avoid including uncommented outputs. Comment on any problems you see with the fit.
- Do the residual plots suggest any unusually large outliers? 
- Does the leverage plot identify any observations with unusually high leverage?
- Do any interactions between horsepower and year appear to be statistically significant?
- Try a transformation of the mpg variable, such as log(X), in order to improve the $R^2_{adj}$. Comment on your findings.


## 3) (*Theory*) 

In SLR, model errors are defined as $$ {e}_{i} = y_i - {y}_i = y_i - (\beta_0 + \beta_1 x_i).$$ The ordinary LS estimation argument with cost function notation can be expressed as $$ \hat{\beta}_{LS}: argmin{J(\beta)} =  argmin{\frac{1}{n}\sum_{1}^{n}{e}_i^2}.$$

a. Obtain the estimating equation for the model parameter $\beta_1$ (using differentiation). If you prefer matrix notation way to obtain the equation in a LR model, this would be great. Then, express the $\hat \beta_1$.

b. In SLR,  is there any difference between $var(\hat\mu_{y_i|x_i})$ and $var(\hat{y}_{x_0})$, where $\hat\mu_{y_i|x_i}$ is estimation at $x_i$ and  $\hat{y}_{x_0}$ is prediction at a future value $x_0$? Explain.

c. `Leverage statistic` of observation $x_i$ on $\hat y$ in a LS regression model is $h_i = H_{ii}$, which describes the degree by which the $i-$th measured value influences the $i$th fitted value. In the slides, we reviewed: $$X \cdot \hat{\beta}=X \cdot (X^t \cdot X)^{-1} \cdot X^t \cdot  y = H \cdot y = \hat y$$ Also, some mathematical properties are expressed as these two arguments: $1/n \leq  h_i \leq 1$, $\bar h = (p+1)/n$. Verify these two formulas numerically using the Model 2 fit in Q2, Auto  dataset. Report the calculations. Comment on the calculations whether or not these are verified.


d. (BONUS) $R^2$ in SLR has two expressions:
$$
R^2 = \frac{\left[ \sum (x_i - \bar{x}) (y_i - \bar{y}) \right]^2}
           {\sum (x_j - \bar{x})^2 \sum (y_k - \bar{y})^2}
$$
and $$ R^2 = \frac{\sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}. $$
Prove that these are equivalent.

e. (BONUS) Ask a challenging question and answer (under the assignment context).

\newpage

***


## Your Solutions

Q1) 

Part a:
```{r}
set.seed(99)
n=100
x1=runif(n) # predictor 1
x2=rnorm(n) # predictor 2
x3=x1+x2+rnorm(n)+5 # predictor 3
y1=2 + 4* x2 + rnorm(n) # Model 1
y2=2 + 3*x1 + 4* x2 + 5*x3 + rnorm(n) # Model 2
lm.fit=lm(y1~x2) #Fitted Model 1
lm.fit2 = lm(y2~x1+x2+x3) #Fitted Model 2
summary(lm.fit) 
plot(x2,y1)
abline(lm(y1~x2),col="red")
coef(lm.fit)
## Results table
est1=summary(lm.fit)$coefficients[1,1]
est2=summary(lm.fit)$coefficients[2,1]
sd1=summary(lm.fit)$coefficients[1,2]
sd2=summary(lm.fit)$coefficients[2,2]
t1=summary(lm.fit)$coefficients[1,3]
t2=summary(lm.fit)$coefficients[2,3]
p1=summary(lm.fit)$coefficients[1,4]
p2=summary(lm.fit)$coefficients[2,4]
A=as.matrix(c(est1,est2))
B=c(sd1,sd2)
C=c(t1,t2)
D=c(p1,p2)
E=cbind(A,B,C,D)
colnames(E)=c("Estimate","Std. Error", "t value", "Pr(>|t|)")
rownames(E)=c("(Intercept)","x2")
E
# Lets use a better format from table
knitr::kable(E, caption = "The regression coefficient estimates")
```
The regression coefficient estimates are 4.02420(slope), and 2.01543(intercept). $y1=4.0242*x2+2.01543$

***
Part b:
```{r}
summary(lm.fit)
summary(lm.fit2)
AIC(lm.fit)
BIC(lm.fit)
AIC(lm.fit2)
BIC(lm.fit2)
```
It's good. Model 1 is better than Model 2 with the comparison of AIC and BIC value (Model 1 has smaller AIC and BIC than that of Model 2), even though its Multiple R-squared is smaller than that of model 2.

***
Part c:
```{r}
summary(lm.fit2)
plot(x1+x2+x3,y2)
abline(lm(y2~x1+x2+x3),col="red")
coef(lm.fit2)
## Results table
est1=summary(lm.fit2)$coefficients[1,1]
est2=summary(lm.fit2)$coefficients[2,1]
est3=summary(lm.fit2)$coefficients[3,1]
est4=summary(lm.fit2)$coefficients[4,1]
sd1=summary(lm.fit2)$coefficients[1,2]
sd2=summary(lm.fit2)$coefficients[2,2]
sd3=summary(lm.fit2)$coefficients[3,2]
sd4=summary(lm.fit2)$coefficients[4,2]
t1=summary(lm.fit2)$coefficients[1,3]
t2=summary(lm.fit2)$coefficients[2,3]
t3=summary(lm.fit2)$coefficients[3,3]
t4=summary(lm.fit2)$coefficients[4,3]
p1=summary(lm.fit2)$coefficients[1,4]
p2=summary(lm.fit2)$coefficients[2,4]
p3=summary(lm.fit2)$coefficients[3,4]
p4=summary(lm.fit2)$coefficients[4,4]
A=as.matrix(c(est1,est2,est3,est4))
B=c(sd1,sd2,sd3,sd4)
C=c(t1,t2,t3,t4)
D=c(p1,p2,p3,p4)
E=cbind(A,B,C,D)
colnames(E)=c("Estimate","Std. Error", "t value", "Pr(>|t|)")
rownames(E)=c("(Intercept)","x1","x2","x3")
E
# Lets use a better format from table
knitr::kable(E, caption = "The regression coefficient estimates")
```
Because all p-values of the estimates(predictors) x1, x2,and x3 are less than 0.05, the predictors are significantly contributing to the model.

***
Part d:
```{r}
cor(x2,x3)
plot(x2, x3, main="Correlation between x2 and x3")
```
The correlation between x2 and x3 is 0.631347. The variables x2 and x3 have fairly strong correlation and are positively correlated.

***
Part e:

Assumptions fits Module 2:

1.Residuals are normally distributed.
```{r}
resi <- resid(lm.fit2) 
hist(resi, main='Histogram of Residual') 
qqnorm(resi)
qqline(resi)
```
According to the histogram and Q-Q plot, we find the residuals have an approximately normal distribution.

***
2.The variance of error terms are similar across the values of the independent variables.
```{r}
library(MASS)
y_hat <- fitted(lm.fit2)
standresid <- stdres(lm.fit2)
plot(y_hat, resi)
plot(y_hat, standresid)
```
According to the plots, the distributions of residuals and standardized residuals are almost the same. Thus, it's reasonable to say that the variances of error terms are similar across the values of the independent variables.

***
3.Independent variables are not strongly correlated with each other.
```{r}
x <- model.matrix(lm.fit2)
co <- ncol(x)
n <- nrow(x)
par(mfrow=c(2,3))
for(i in 2:co){plot(x[,i],standresid, xlab=paste0("x",i),
                   ylab="Standardized Residuals")
abline(h=c(-2,2),col=i)}
```
Because the three graphs show random distributions with different patterns, any of the two independent variables are not highly correlated.

***
4.A linear relationship between the response variable and the independent variables.
```{r}
plot(1:n, standresid)
```
We can see there is no specific pattern in the residual graph, so the relationship between the response variable and the independent variables could be linearly related.

***
Part f:
```{r}
lm.fit3=lm(y1~x1+x2+x3) #Changed Model 1
summary(lm.fit3)
lm.fit=lm(y1~x2) #Fitted Model 1
anova(lm.fit,lm.fit3)
```
Because the p-value is larger than 0.05, Module 1 is not improved by adding the new directors x1 and x3.

***
Part g:
```{r}
new_y2 <- y2
new_y2[1]=100+100*y2[1]
cat("The new observation value is", new_y2[1])
summary(new_y2)
lm.fit4=lm(new_y2~x1+x2+x3)
summary(lm.fit4)
plot(x1+x2+x3,new_y2)
```
In Coefficients, the estimates of intercept and x1 extremely increase, while the estimates of x2 and x3 decrease. The standard errors of all elements (intercept,x1,x2,x3) increase. T values of all elements (intercept,x1,x2,x3) decrease. Residual of standard error extremely increases. F statistic extremely decreases. R-Square decreases and even become negative. P-value largely increases. This new observation new_y2[1] is an outlier on the fitted model because it's in the position far larger than $1.5*IQR$ from Q3, and it's also a high-leverage point since the estimate of x1 becomes larger as the estimates of x2 and x3 decrease to negative.

***
Part h:
```{r}
y2=2 + 3*x1 + 4* x2 + 5*x3 + rnorm(n) # Model 2
new_x1 <- x1
new_x1[1]=100+100*x1[1]
cat("The new observation value is", new_x1[1])
summary(new_x1)
lm.fit5=lm(y2~new_x1+x2+x3)
summary(lm.fit5)
plot(new_x1+x2+x3,y2)
```
In Coefficients, the estimate of x1 decreases, while the estimates of intercept and x3 increases a little(almost the same as before), estimate of x2 decreases a little(almost the same as before). The standard errors of intercept,x2,x3 just change a little, while the standard error of x1 decreases. T values of all x1,x2,x3 decrease, while T value of intercept little increases. The P-values of intercept and x1 increase. Residual of standard error increases a little. F statistic decreases. R-Square decreases a little and p-value keeps almost the same. This new observation new_x1[1] is an outlier on the fitted model because it's in the position larger than $1.5*IQR$ from Q3, and it's a high-leverage point since the estimate of x1 is obviously smaller than the others. The affects of corrupted data on model estimates in part h is not as strong as the influence in part g. Partly because the new observation new_x1[1] of part h just influences an independent variable x1 while the new observation new_y2[1] of part g affects the response variable which are more likely to significantly influence the efficiency of the model.

***


\newpage

Q2) 

Part a:
```{r}
attach(Auto) #this enables to use the column names
summary(Auto) #always do EDA and graphs first

name <- as.numeric(factor(name))
MBaseline <- mean(mpg) # Model Beasline
M1 <- lm(mpg~horsepower) # Model 1
M2 <- lm(mpg~horsepower+year) # Model 2
M3 <- lm(mpg~.-origin-year-name,data=Auto) # Model 3
MFull <- lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin+name) # Model Full
                
library(ISLR)
pairs(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin+name,pch=".")
```

***
Part b:
```{r}
cor_Auto <- matrix(cor(as.matrix(Auto[1:6])),6,6)
colnames(cor_Auto)=c("mpg","cylinders","displacement","horsepower","weight","acceleration")
rownames(cor_Auto)=c("mpg","cylinders","displacement","horsepower","weight","acceleration")
knitr::kable(cor_Auto, caption = "Matrix of correlations")
```

***
Part c:
```{r}
coef(M1) # coefficient in Model 1
cat('The coefficient for the horsepower in Model 1 is', -0.1578447)
coef(M2) # coefficient in Model 2
cat('The coefficient for the horsepower in Model 2 is', -0.1316544)
coef(M3) # coefficient in Model 3
cat('The coefficient for the horsepower in Model 3 is', -0.016951144)
coef(MFull) # coefficient in Model Full
cat('The coefficient for the horsepower in Model Full is', -0.042342228)
```
The coefficient for the horsepower variable in Model 1 suggests that for each unit increase in engine horsepower, the car will run fewer 0.1578447 miles per gallon. The coefficient for the horsepower variable changes in other models. Compared with the Model 2, the coefficient for the horsepower does not change so much. But as more different independent variables added to the multiple linear regression model just as Model 3 and Model Full, the coefficient for horsepower changes to smaller and smaller.

***
Part d:
```{r}
# Model 1
X <- model.matrix(M1)
n <- nrow(X)
y <- mpg
yhat <- fitted(M1)
res <- resid(M1)
SSE_M1=sum(res^2) 
J <- matrix(rep(1,n*n),nrow=n)
k = dim(X)[2]
MSE_M1= SSE_M1/(n-k)
SSTO_M1 <- t(y)%*%y -(1/n)*t(y)%*%J%*%y 
r2_1=summary(M1)$r.squared 
adj_r2_1=summary(M1)$adj.r.squared 
bic1=BIC(M1) 
F_ts1=summary(M1)$fstatistic[1] 
F_p1=anova(M1)$'Pr(>F)'[1] 

#Model 2
X <- model.matrix(M2)
n <- nrow(X)
y <- mpg
yhat <- fitted(M2)
res <- resid(M2)
SSE_M2=sum(res^2) 
J <- matrix(rep(1,n*n),nrow=n)
k = dim(X)[2]
MSE_M2= SSE_M2/(n-k) 
SSTO_M2 <- t(y)%*%y -(1/n)*t(y)%*%J%*%y 
r2_2=round(summary(M2)$r.squared,4) 
adj_r2_2=summary(M2)$adj.r.squared 
bic2=BIC(M2) 
F_ts2 <- summary(M2)$fstatistic[1] 
F_p2=anova(M2)$'Pr(>F)'[1] 

#Model 3
X <- model.matrix(M3)
n <- nrow(X)
yhat <- fitted(M3)
res <- resid(M3)
SSE_M3=sum(res^2) 
J <- matrix(rep(1,n*n),nrow=n)
k = dim(X)[2]
MSE_M3= SSE_M3/(n-k) 
SSTO_M3 <- t(y)%*%y -(1/n)*t(y)%*%J%*%y 
r2_3=summary(M3)$r.squared 
adj_r2_3=summary(M3)$adj.r.squared 
bic3=BIC(M3) 
F_ts3=summary(M3)$fstatistic[1] 
F_p3=anova(M3)$'Pr(>F)'[1] 

# Model Full
X <- model.matrix(MFull)
n <- nrow(X)
yhat <- fitted(MFull)
res <- resid(MFull)
SSE_M4=sum(res^2) 
J <- matrix(rep(1,n*n),nrow=n)
k = dim(X)[2]
SSTO_M4 <- t(y)%*%y -(1/n)*t(y)%*%J%*%y 
MSE_M4= SSE_M4/(n-k) 
r2_4=summary(MFull)$r.squared 
adj_r2_4=summary(MFull)$adj.r.squared 
bic4=BIC(MFull) 
F_ts4=summary(MFull)$fstatistic[1] 
F_p4=anova(MFull)$'Pr(>F)'[1] 

#make the table 
W=as.matrix(c(SSTO_M1,SSTO_M2,SSTO_M3,SSTO_M4)) 
A=c(MSE_M1,MSE_M2,MSE_M3,MSE_M4)
B=c(r2_1,r2_2,r2_3,r2_4)
C=c(adj_r2_1,adj_r2_2,adj_r2_3,adj_r2_4)
D=c(bic1,bic2,bic3,bic4)
E=c(F_ts1,F_ts2,F_ts3,F_ts4)
G=c(F_p1,F_p2,F_p3,F_p4)
table=cbind(W,A,B,C,D,E,G)
colnames(table)=c("SSTO","MSE","R-squared","R-squared_adj","BIC","F-ts","F-pvalue")
rownames(table)=c("Model 1","Model 2","Model 3","Model Full")
table
knitr::kable(table, caption = "Table of Statistics for each model")
cat('The measurement of Model Baseline is not applicable.')
```

***
Part e:

Here, Model Full is the best in quality while Model 1 is simple and not that complicated.
In the above table we can see that as the model includes more variables, the R-squared becomes larger and larger and p-value changes to smaller and smaller. It means that the more variance in response variable could be explained by the model of the relationship between independent variables and the outcome variable. 

***
Part f:
```{r}
library(lm.beta)
lm.beta(lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin+name))
```
In the summary, the predictor "weight" appears most important in the Model Full because it has the largest absolute value about the standardized coefficients, which means it has the largest mean change in the response variable mpg with a change of one standard deviation.

***
Part g:
```{r}
predict(M2,newdata = data.frame(horsepower=200,year=80), interval="predict")
```
The 95% prediction interval for the prediction using Model 2 is (4.745447, 22.2773).

***
Part h:
```{r}
residual <- resid(M2)
standard_residual <- stdres(M2)
y_hat <- fitted(M2)
par(mfrow=c(2,2)) 
plot(M2)
plot(y_hat, residual)
plot(y_hat, standard_residual)
plot(hatvalues(M2))

X <- model.matrix(M2)
co <- ncol(X)
n <- nrow(X)
par(mfrow=c(2,3))
for(i in 2:co){plot(X[,i],standard_residual,xlab=paste0("X",i),ylab="Standardized Residuals")
abline(h=c(-2,2),col=i)}
plot(1:n, standard_residual)

summary(M2)
library(car)
vif(M2)
M2_new <- lm(mpg~log(horsepower)+year)
summary(M2_new)
```
The graphs of residuals and standardized residuals with y_hat look almost the same pattern and show equal variance. Normal Q-Q plot shows that the residuals have approximately normal distribution.The 1:n to standard_residual graph is randomly bounced around 0, which means that there could be a linear relationship between the response variable and the independent variables. The graphs of standardized residuals about two predictors show that the variance of error terms are similar across the values of the independent variables. We didn't see unusually large outlier in the residuals vs. fitted graph. In the residuals vs. leverage plot, there is also no unusually high leverage. Because the value we get from vif is small, the interactions between horsepower and year are not statistically significant. With log(X) transformation, the new multiple r-squared and adjusted R-squared are larger than before. Residual standard error decreases. This means regression model fits the observed data better than before.

***
Part i:
```{r}
predict(M2,newdata=data.frame(horsepower=200,year=80), interval="confidence")
```
The 95% confidence interval for the estimation is (11.96143, 15.06132). The interval in part g (4.745447, 22.2773) is wider than that in i (11.96143, 15.06132). The prediction interval predicts in what range a future individual observation will fall, while a confidence interval shows the likely range of values associated with some statistical parameter of the data. Therefore, the prediction interval in part g are measuring the future values of mpg. Rather than just focusing on current values, there will be new standard deviations of error terms with the future observations of horsepower and years.

\newpage


Q3) 

Part a:

The answering processes are in the HW-Q3a&d-Xubin_Lou.pdf file as I submitted to the blackboard, because I wrote it down by hand.

***
Part b:

There is a difference between the two variances. To estimate its corresponding y value in the estimation at $x_i$ focusing on the current values of x, we assume the error term would be 0. But in the prediction at a future value $x_0$, there would be a new standard deviation of the error term that making the point more deviated from the regression line as there's no such assumption that the error term is to be zero. 

***
Part c:
```{r}
X <- model.matrix(M2)
p <- ncol(X)
n <- nrow(X)
H <- X%*%solve(t(X)%*%X)%*%t(X)
hii <- diag(H)
cat('1/n equals',1/n)
cat('Maximum of h statistics is', max(hii))
cat('Minimum of h statistics is', min(hii))
plot(1:n, hii)
text(1:n-2, hii, labels=seq(1:n)) #-2 for better see
cat('Mean of h statistics is', mean(hii))
cat('(p+1)/n equals', (p+1)/n)
```
The maximum of h is 0.03068558; the minimum of h is 0.0025519. 1/n is 0.00255102 which is smaller than the minimum of h 0.0025519. The maximum of h 0.03068558 is smaller than 1. The first formula is verified. The mean of h statistics is 0.007653061 and (p+1)/n is around 0.01020408. Thus, the second formula is not verified because the mean and (p+1)/n are not equal.

***
Part d (BONUS):

The answering processes are in the HW-Q3a&d-Xubin_Lou.pdf file as I submitted to the blackboard, because I wrote it down by hand.

***
Part e (BONUS):


***


\newpage

### Write comments, questions: ...


***
I hereby write and submit my solutions without violating the academic honesty and integrity. If not, I accept the consequences. 

### List the fiends you worked with (name, last name): 
Rong Fan, Qihang Tang, Jiayue Meng

### Disclose the resources or persons if you get any help:
Module1_Lab_Advanced_And_SomeHWHelp.r

Module1_Lab_RCodes.r


### How long did the assignment solutions take?: 
About 12 hours I think.


***
## References
Module1_Lab_Advanced_And_SomeHWHelp.r

Module1_Lab_RCodes.r
...
